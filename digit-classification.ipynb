{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use data from the standard MNIST set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "require 'torch'\n",
    "require 'nn'\n",
    "require 'optim'\n",
    "mnist = require 'mnist'\n",
    "require 'cutorch'\n",
    "require 'cudnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fullset = mnist.traindataset()\n",
    "testset = mnist.testdataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fullset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We inspect the data just to get an idea of the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itorch.image(fullset.data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fullset.label[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can split the full dataset into a trainin component and a validation component, which will be used to train hyperparameters.\n",
    "\n",
    "While doing so, we convert the dataset to double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainset = {\n",
    "    size = 50000,\n",
    "    data = fullset.data[{{1,50000}}]:double(),\n",
    "    label = fullset.label[{{1,50000}}]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validationset = {\n",
    "    size = 10000,\n",
    "    data = fullset.data[{{50001,60000}}]:double(),\n",
    "    label = fullset.label[{{50001,60000}}]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a model with a single hidden layer, using a hyperbolic tangent activation, and a softmax output. We also use a first layer to reshape the input - which is a 28x28 square - to fit into the linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model:add(nn.Reshape(28*28))\n",
    "model:add(nn.Linear(28*28, 30))\n",
    "model:add(nn.Tanh())\n",
    "model:add(nn.Linear(30, 10))\n",
    "model:add(nn.LogSoftMax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a loss function, using the negative log likelihood criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion = nn.ClassNLLCriterion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in [the documentation](https://github.com/torch/nn/blob/master/doc/criterion.md), the NLL criterion require the output of the neural network to contain log-probabilities of each class, and this is the reason for our use of `LogSoftMax` above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of the `optim` package to train the network. `optim` contains several optimization algorithms. All of these algorithms assume the same parameters:\n",
    "\n",
    "- a closure that computes the loss, and its gradient wrt to `x`, given a point `x`\n",
    "- a point x\n",
    "- some parameters, which are algorithm-specific\n",
    "\n",
    "We define a `step` function that performs training for a single epoch and returns the current loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd_params = {\n",
    "   learningRate = 1e-2,\n",
    "   learningRateDecay = 1e-4,\n",
    "   weightDecay = 1e-3,\n",
    "   momentum = 1e-4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, dl_dx = model:getParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "step = function(batch_size)\n",
    "    local current_loss = 0\n",
    "    local count = 0\n",
    "    local shuffle = torch.randperm(trainset.size)\n",
    "    batch_size = batch_size or 200\n",
    "    \n",
    "    for t = 1,trainset.size,batch_size do\n",
    "        -- setup inputs and targets for this mini-batch\n",
    "        local size = math.min(t + batch_size - 1, trainset.size) - t\n",
    "        local inputs = torch.Tensor(size, 28, 28)\n",
    "        local targets = torch.Tensor(size)\n",
    "        for i = 1,size do\n",
    "            local input = trainset.data[shuffle[i+t]]\n",
    "            local target = trainset.label[shuffle[i+t]]\n",
    "            -- if target == 0 then target = 10 end\n",
    "            inputs[i] = input\n",
    "            targets[i] = target\n",
    "        end\n",
    "        targets:add(1)\n",
    "        \n",
    "        local feval = function(x_new)\n",
    "            -- reset data\n",
    "            if x ~= x_new then x:copy(x_new) end\n",
    "            dl_dx:zero()\n",
    "\n",
    "            -- perform mini-batch gradient descent\n",
    "            local loss = criterion:forward(model:forward(inputs), targets)\n",
    "            model:backward(inputs, criterion:backward(model.output, targets))\n",
    "\n",
    "            return loss, dl_dx\n",
    "        end\n",
    "        \n",
    "        _, fs = optim.sgd(feval, x, sgd_params)\n",
    "        -- fs is a table containing value of the loss function\n",
    "        -- (just 1 value for the SGD optimization)\n",
    "        count = count + 1\n",
    "        current_loss = current_loss + fs[1]\n",
    "    end\n",
    "\n",
    "    -- normalize loss\n",
    "    return current_loss / count\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the training, we also need to be able to evaluate accuracy on a separate dataset, in order to define when to stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval = function(dataset, batch_size)\n",
    "    local count = 0\n",
    "    batch_size = batch_size or 200\n",
    "    \n",
    "    for i = 1,dataset.size,batch_size do\n",
    "        local size = math.min(i + batch_size - 1, dataset.size) - i\n",
    "        local inputs = dataset.data[{{i,i+size-1}}]\n",
    "        local targets = dataset.label[{{i,i+size-1}}]:long()\n",
    "        local outputs = model:forward(inputs)\n",
    "        local _, indices = torch.max(outputs, 2)\n",
    "        indices:add(-1)\n",
    "        local guessed_right = indices:eq(targets):sum()\n",
    "        count = count + guessed_right\n",
    "    end\n",
    "\n",
    "    return count / dataset.size\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to perform the actual training. After each epoch, we evaluate the accuracy on the validation dataset, in order to decide whether to stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iters = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "do\n",
    "    local last_accuracy = 0\n",
    "    local decreasing = 0\n",
    "    local threshold = 1 -- how many deacreasing epochs we allow\n",
    "    for i = 1,max_iters do\n",
    "        local loss = step()\n",
    "        print(string.format('Epoch: %d Current loss: %4f', i, loss))\n",
    "        local accuracy = eval(validationset)\n",
    "        print(string.format('Accuracy on the validation set: %4f', accuracy))\n",
    "        if accuracy < last_accuracy then\n",
    "            if decreasing > threshold then break end\n",
    "            decreasing = decreasing + 1\n",
    "        else\n",
    "            decreasing = 0\n",
    "        end\n",
    "        last_accuracy = accuracy\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the model accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testset.data = testset.data:double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Saving and restoring the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `paths` module can be used to manipulate filesystem paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paths = require 'paths'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = paths.concat(paths.cwd(), 'model.net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then save our model to file like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(torch.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(filename, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check that restoring from file works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(torch.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1 = torch.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We redefine our evaluation function to use the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval1 = function(dataset)\n",
    "   local count = 0\n",
    "   for i = 1,dataset.size do\n",
    "      local output = model1:forward(dataset.data[i])\n",
    "      local _, index = torch.max(output, 1) -- max index\n",
    "      local digit = index[1] % 10\n",
    "      if digit == dataset.label[i] then count = count + 1 end\n",
    "   end\n",
    "\n",
    "   return count / dataset.size\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval1(testset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
